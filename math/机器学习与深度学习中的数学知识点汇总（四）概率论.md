# 机器学习与深度学习中的数学知识点汇总（四）

来源：公众号SIGAI

本文仅用于学习交流分享，如有侵权请联系删除

------

### 概率论与信息论

概率论与信息论在机器学习中用得非常多。概率论的知识，一般不超出工科教材的范畴。而信息论是很多同学没有学过的，不过只要你理解了微积分和概率论，理解这些概念并不是难事。下面列出常用的概率论与信息论知识点。

**随机事件与概率。**这是理解随机变量的基础，也是概率论中最基本的知识。

**条件概率与独立性。**条件概率非常重要，在机器学习中，只要有概率模型的地方，通常离不开它。独立性在很多地方也被使用，如概率论图模型。

**条件独立。**在概率论图模型中广泛使用，一定要理解它。

**全概率公式。**基础公式，地位不用多说。

**贝叶斯公式。**在机器学习的概率型算法中处于灵魂地位，几乎所有生成模型都要用到它。

**离散型随机变量与连续型随机变量。**重要性不用多说，概率质量函数，概率密度函数，分布函数，一定要熟练掌握。

**数学期望。**非常重要，好多地方都有它的影子。

方差与标准差。非常重要，刻画概率分布的重要指标。

**Jensen不等式。**在很多推导和证明中都要用它，如EM算法，变分推断。

**常用的概率分布**，包括均匀分布，正态分布，伯努利分布，二项分布，多项分布，t分布等，在各种机器学习算法中广泛使用。

**随机向量。**多元的随机变量，在实际中更有用。

**协方差**。经常使用的一个概念，如主成分分析，多元正态分布中。

**参数估计**。包括最大似然估计，最大后验概率估计，贝叶斯估计，核密度估计，一定要弄清楚它们是怎么回事。

**随机算法**，包括采样算法，遗传算法，蒙特卡洛算法，在机器学习中也经常使用。

**信息论中的一些概念**，包括熵，交叉熵，KL散度，JS散度，互信息，信息增益，一定要深刻理解这些概念。如果你不理解KL散度，那怎么理解变分推断和VAE？

参考书目：

概率论国内理工科专业使用最多的是浙大版的教材：

《概率论与数理统计》，国外的书籍推荐《信息论基础》

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACmc0q15IOmjGSbRDjLfsbVcpMkcKTibYibnr8EpczlSdJt2q8nibBkAkooHAATJPq6icwRdT2MJZ0yzqw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)